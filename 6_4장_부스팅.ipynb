{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6-4장. 부스팅.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMldgGs/GftYDvarajpAtlg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimYongHwi/ml-perfect-guide-study/blob/main/6_4%EC%9E%A5_%EB%B6%80%EC%8A%A4%ED%8C%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG4x6L00CJr_"
      },
      "source": [
        "### 부스팅(Boosting)\n",
        "\n",
        "- 부스팅 알고리즘은 여러 개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식이다.\n",
        "\n",
        "- 부스팅의 대표적인 구현은 AdaBoost(Adaptive Boosting)와 그레디언트 부스트가 있다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGu19Tp2DF3A"
      },
      "source": [
        "### GBM (Gradient Boost Machine) 개요\n",
        "\n",
        "GBM도 AdaBoost와 유사하나, 가중치 업데이트를 경사 하강법(Gradient Descent)을 이용하는것이 큰 차이다. 오류 값은 `실제 값 - 예측 값`으로 분류의 실제 결과값을 $y$, feature를 $X_1, X_2, X_3, ... X_n$, 그리고 해당 feature에 기반한 예측 함수를 $F(x)$ 함수라고 하면 오류식 $h(x) = y - F(x)$가 된다. 이 오류식을 최소화하는 방향성을 가지고 반복적으로 가중치 값을 업데이트하는 것이 경사 하강법이다. 경사 하강법은 반복 수행을 통해 오류를 최소화할 수 있도록 가중치의 업데이트 값을 도출하는 기법으로서 머신러닝에서 중요한 기법 중 하나이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpzDcetUDprM",
        "outputId": "d32b06cb-0bc2-463b-c91f-87f78f4975da"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLYGJTCyD1EQ"
      },
      "source": [
        "import os\n",
        "from pathlib import Path"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOUV0fGWD2Ze",
        "outputId": "cd5930ed-cf3c-49c5-c43e-d3f6d37a2ca4"
      },
      "source": [
        "folder = \"ml_definitive_guide/data/human_activity_dataset\"\n",
        "\n",
        "base_path = Path(\"/content/drive/My Drive/\")\n",
        "project_path = base_path / folder\n",
        "os.chdir(project_path)\n",
        "\n",
        "for x in list(project_path.glob(\"*\")):\n",
        "    if x.is_dir():\n",
        "        dir_name = str(x.relative_to(project_path))\n",
        "        os.rename(dir_name, dir_name.split(\" \", 1)[0])\n",
        "\n",
        "print(f\"현재 디렉토리 위치: {os.getcwd()}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 디렉토리 위치: /content/drive/My Drive/ml_definitive_guide/data/human_activity_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a1G7d2f-Jk2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# features.txt 파일에 있는 컬럼명을 입력 받아서 중복된 컬럼명은 \n",
        "# 원본 컬럼명+_1, _2와 같이 중복된 차수를 원본 컬럼명에 더해서 컬럼명을 update 하는 함수\n",
        "def get_new_feature_name_df(old_feature_name_df):\n",
        "    #column_name으로 중복된 컬럼명에 대해서는 중복 차수 부여, col1, col1과 같이 2개의 중복 컬럼이 있을 경우 1, 2 \n",
        "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt'])\n",
        "\n",
        "    # feature_dup_df의 index인 column_name을 reset_index()를 이용하여 컬럼으로 변환. \n",
        "    feature_dup_df = feature_dup_df.reset_index()\n",
        "\n",
        "    # 인자로 받은 features_txt의 컬럼명 DataFrame과 feature_dup_df를 조인. \n",
        "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')\n",
        "\n",
        "    # 새로운 컬럼명은 앞에 중복 차수를 접미어로 결합. \n",
        "    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) \n",
        "                                                                                           if x[1] >0 else x[0] ,  axis=1)\n",
        "    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)\n",
        "    return new_feature_name_df\n",
        "\n",
        "def get_human_dataset( ):\n",
        "    \n",
        "    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.\n",
        "    feature_name_df = pd.read_csv(\n",
        "        './features.txt', \n",
        "        sep='\\s+', header=None, names=['column_index', 'column_name'])\n",
        "    \n",
        "    # 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성. \n",
        "    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
        "    \n",
        "    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
        "    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
        "    \n",
        "    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용\n",
        "    X_train = pd.read_csv('./train/X_train.txt',sep='\\s+', names=feature_name )\n",
        "    X_test = pd.read_csv('./test/X_test.txt',sep='\\s+', names=feature_name)\n",
        "    \n",
        "    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여\n",
        "    y_train = pd.read_csv('./train/y_train.txt',sep='\\s+',header=None,names=['action'])\n",
        "    y_test = pd.read_csv('./test/y_test.txt',sep='\\s+',header=None,names=['action'])\n",
        "    \n",
        "    # 로드된 학습/테스트용 DataFrame을 모두 반환 \n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDZP2FJUFRq7",
        "outputId": "e7469f90-c11b-4c86-efeb-66e776cf544a"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "X_train, X_test, y_train, y_test = get_human_dataset()\n",
        "\n",
        "# GBM 수행 시간 측정을 위함. 시작 시간 설정.\n",
        "start_time = time.time()\n",
        "\n",
        "gb_clf = GradientBoostingClassifier(random_state=0)\n",
        "gb_clf.fit(X_train , y_train)\n",
        "gb_pred = gb_clf.predict(X_test)\n",
        "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
        "\n",
        "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))\n",
        "print(\"GBM 수행 시간: {0:.1f} 초 \".format(time.time() - start_time))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GBM 정확도: 0.9386\n",
            "GBM 수행 시간: 649.8 초 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-xQXMF0Fd6g",
        "outputId": "94d1d833-24c0-4a7d-d6c4-da19bd9c9878"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'n_estimators':[100, 500],\n",
        "    'learning_rate' : [ 0.05, 0.1]\n",
        "}\n",
        "grid_cv = GridSearchCV(gb_clf , param_grid=params , cv=2 ,verbose=1, n_jobs=-1)\n",
        "grid_cv.fit(X_train , y_train)\n",
        "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
        "print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed: 52.5min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최적 하이퍼 파라미터:\n",
            " {'learning_rate': 0.05, 'n_estimators': 500}\n",
            "최고 예측 정확도: 0.9013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "742m4bifJpTF"
      },
      "source": [
        "### XGBoost (eXtra Gradient Boost)\n",
        "\n",
        "- 뛰어난 예측 성능\n",
        "- GBM 대비 빠른 수행 시간\n",
        "  - CPU 병렬 처리, GPU 지원\n",
        "- 다양한 성능 향상 기능\n",
        "  - Regularization 기능 탑재\n",
        "  - Tree Pruning\n",
        "- 다양한 편의 기능\u001c\n",
        "  - 조기 중단\n",
        "  - 자체 내장된 교차 검증\n",
        "  - 결손값 자체처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saUZLEuBJkFc"
      },
      "source": [
        "import xgboost"
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}